{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dd23a5",
   "metadata": {},
   "source": [
    "# Project 2. Submitted By Hrishikesh Ramekar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c53c2",
   "metadata": {},
   "source": [
    "# Income Qualification\n",
    "## DESCRIPTION\n",
    "Identify the level of income qualification needed for the families in Latin America.\n",
    "\n",
    "Problem Statement Scenario: Many social programs have a hard time ensuring that the right people are given enough aid. It’s tricky when a program focuses on the poorest segment of the population. This segment of the population can’t provide the necessary income and expense records to prove that they qualify.\n",
    "\n",
    "In Latin America, a popular method called Proxy Means Test (PMT) uses an algorithm to verify income qualification. With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling or the assets found in their homes to classify them and predict their level of need.\n",
    "\n",
    "While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.\n",
    "\n",
    "The Inter-American Development Bank (IDB)believes that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7efa38",
   "metadata": {},
   "source": [
    "## Following actions should be performed:\n",
    "1.Identify the output variable.  \n",
    "2.Understand the type of data.  \n",
    "3.Check if there are any biases in your dataset.  \n",
    "4.Check whether all members of the house have the same poverty level.  \n",
    "5.Check if there is a house without a family head.  \n",
    "6.Set poverty level of the members and the head of the house within a family.  \n",
    "7.Count how many null values are existing in columns.  \n",
    "8.Remove null value rows of the target variable.  \n",
    "9.Predict the accuracy using random forest classifier.  \n",
    "10.Check the accuracy using random forest with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4f1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b48446",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-100d2f30842f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_income_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_income_test\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df_income_train = pd.read_csv(\"train.csv\")\n",
    "df_income_test =  pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150fc7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af109d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### List the columns for different datatypes:\n",
    "print('Integer Type: ')\n",
    "print(df_income_train.select_dtypes(np.int64).columns)\n",
    "print('\\n')\n",
    "print('Float Type: ')\n",
    "print(df_income_train.select_dtypes(np.float64).columns)\n",
    "print('\\n')\n",
    "print('Object Type: ')\n",
    "print(df_income_train.select_dtypes(np.object).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ac01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.select_dtypes('int64').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find columns with null values\n",
    "null_counts=df_income_train.select_dtypes('int64').isnull().sum()\n",
    "null_counts[null_counts > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfeb8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.select_dtypes('float64').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find columns with null values\n",
    "null_counts=df_income_train.select_dtypes('float64').isnull().sum()\n",
    "null_counts[null_counts > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18322669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.select_dtypes('object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find columns with null values\n",
    "null_counts=df_income_train.select_dtypes('object').isnull().sum()\n",
    "null_counts[null_counts > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20f9d8",
   "metadata": {},
   "source": [
    "\n",
    "Looking at the different types of data and null values for each feature. We found the following:\n",
    "\n",
    "No null values for Integer type features.\n",
    "No null values for object type features.\n",
    "For float64 types below featufres has null value\n",
    "A.v2a1 6860\n",
    "B.v18q1 7342\n",
    "C.rez_esc 7928\n",
    "D.meaneduc 5\n",
    "E.SQBmeaned 5\n",
    "We also noticed that object type features dependency, edjefe, edjefa have mixed values.\n",
    "Lets fix the data for features with null values and features with mixed values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7d9c1",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fa992",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={'yes':1,'no':0}\n",
    "\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['dependency'] =df['dependency'].replace(mapping).astype(np.float64)\n",
    "    df['edjefe'] =df['edjefe'].replace(mapping).astype(np.float64)\n",
    "    df['edjefa'] =df['edjefa'].replace(mapping).astype(np.float64)\n",
    "    \n",
    "df_income_train[['dependency','edjefe','edjefa']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8b21d",
   "metadata": {},
   "source": [
    "Lets fix the column with null values\n",
    "According to the documentation for these columns:\n",
    "\n",
    "v2a1 (total nulls: 6860) : Monthly rent payment\n",
    "v18q1 (total nulls: 7342) : number of tablets household owns\n",
    "rez_esc (total nulls: 7928) : Years behind in school\n",
    "meaneduc (total nulls: 5) : average years of education for adults (18+)\n",
    "SQBmeaned (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142\n",
    "\n",
    "Lets look at v2a1 (total nulls: 6860) : Monthly rent payment\n",
    "\n",
    "why the null values, Lets look at few rows with nulls in v2a1:\n",
    "\n",
    "1.Columns related to Monthly rent payment\n",
    "2.tipovivi1, =1 own and fully paid house\n",
    "3.tipovivi2, \"=1 own, paying in installments\"\n",
    "4.tipovivi3, =1 rented\n",
    "5.tipovivi4, =1 precarious\n",
    "6.tipovivi5, \"=1 other(assigned, borrowed)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25225550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_income_train[df_income_train['v2a1'].isnull()].head()\n",
    "\n",
    "columns=['tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5']\n",
    "data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables indicating home ownership\n",
    "own_variables = [x for x in df_income_train if x.startswith('tipo')]\n",
    "\n",
    "\n",
    "# Plot of the home ownership variables for home missing rent payments\n",
    "df_income_train.loc[df_income_train['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n",
    "                                                                        color = 'green',\n",
    "                                                              edgecolor = 'k', linewidth = 2);\n",
    "plt.xticks([0, 1, 2, 3, 4],\n",
    "           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n",
    "          rotation = 20)\n",
    "plt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the above data it makes sense that when the house is fully paid, there will be no monthly rent payment.\n",
    "#Lets add 0 for all the null values.\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['v2a1'].fillna(value=0, inplace=True)\n",
    "\n",
    "df_income_train[['v2a1']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72d57a",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at v18q1 (total nulls: 7342) : number of tablets household owns\n",
    "why the null values, Lets look at few rows with nulls in v18q1\n",
    "Columns related to number of tablets household owns\n",
    "v18q, owns a tablet\n",
    "\n",
    "Since this is a household variable, it only makes sense to look at it on a household level, so we'll only select the rows for the head of household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heads of household### NOTE\n",
    "heads = df_income_train.loc[df_income_train['parentesco1'] == 1].copy()\n",
    "heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d56ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "col='v18q1'\n",
    "df_income_train[col].value_counts().sort_index().plot.bar(color = 'blue',\n",
    "                                             edgecolor = 'k',\n",
    "                                             linewidth = 2)\n",
    "plt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe94c86",
   "metadata": {},
   "source": [
    "Looking at the above data it makes sense that when owns a tablet column is 0, there will be no number of tablets household owns. Lets add 0 for all the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_income_train, df_income_test]:\n",
    "    df['v18q1'].fillna(value=0, inplace=True)\n",
    "\n",
    "df_income_train[['v18q1']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634af065",
   "metadata": {},
   "source": [
    "Lets look at rez_esc (total nulls: 7928) : Years behind in school\n",
    "why the null values, Lets look at few rows with nulls in rez_esc\n",
    "Columns related to Years behind in school\n",
    "Age in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the data with not null values first.\n",
    "df_income_train[df_income_train['rez_esc'].notnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759c1bd",
   "metadata": {},
   "source": [
    "\n",
    "From the above , we see that when min age is 7 and max age is 17 for Years, then the 'behind in school' column has a value.\n",
    "Lets confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a22959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.loc[df_income_train['rez_esc'].isnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.loc[(df_income_train['rez_esc'].isnull() & \n",
    "                     ((df_income_train['age'] > 7) & (df_income_train['age'] < 17)))]['age'].describe()\n",
    "#There is one value that has Null for the 'behind in school' column with age between 7 and 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train[(df_income_train['age'] ==10) & df_income_train['rez_esc'].isnull()].head()\n",
    "df_income_train[(df_income_train['Id'] =='ID_f012e4242')].head()\n",
    "#there is only one member in household for the member with age 10 and who is 'behind in school'. This explains why the member is \n",
    "#behind in school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f88644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from above we see that  the 'behind in school' column has null values \n",
    "# Lets use the above to fix the data\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['rez_esc'].fillna(value=0, inplace=True)\n",
    "df_income_train[['rez_esc']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e2dbb",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at meaneduc (total nulls: 5) : average years of education for adults (18+)\n",
    "why the null values, Lets look at few rows with nulls in meaneduc\n",
    "Columns related to average years of education for adults (18+)\n",
    "edjefe, years of education of male head of household, based on the interaction of escolari (years of education),\n",
    "head of household and gender, yes=1 and no=0\n",
    "edjefa, years of education of female head of household, based on the interaction of escolari (years of education),\n",
    "head of household and gender, yes=1 and no=0\n",
    "instlevel1, =1 no level of education\n",
    "instlevel2, =1 incomplete primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_income_train[df_income_train['meaneduc'].isnull()].head()\n",
    "\n",
    "columns=['edjefe','edjefa','instlevel1','instlevel2']\n",
    "data[columns][data[columns]['instlevel1']>0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above, we find that meaneduc is null when no level of education is 0\n",
    "#Lets fix the data\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['meaneduc'].fillna(value=0, inplace=True)\n",
    "df_income_train[['meaneduc']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61dabc",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at SQBmeaned (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142\n",
    "why the null values, Lets look at few rows with nulls in SQBmeaned\n",
    "Columns related to average years of education for adults (18+)\n",
    "edjefe, years of education of male head of household, based on the interaction of escolari (years of education),\n",
    "head of household and gender, yes=1 and no=0\n",
    "edjefa, years of education of female head of household, based on the interaction of escolari (years of education),\n",
    "head of household and gender, yes=1 and no=0\n",
    "instlevel1, =1 no level of education\n",
    "instlevel2, =1 incomplete primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_income_train[df_income_train['SQBmeaned'].isnull()].head()\n",
    "\n",
    "columns=['edjefe','edjefa','instlevel1','instlevel2']\n",
    "data[columns][data[columns]['instlevel1']>0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above, we find that SQBmeaned is null when no level of education is 0\n",
    "#Lets fix the data\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['SQBmeaned'].fillna(value=0, inplace=True)\n",
    "df_income_train[['SQBmeaned']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1731ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the overall data\n",
    "null_counts = df_income_train.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check one household\n",
    "df_income_train[df_income_train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use Target value of the parent record (head of the household) and update rest. But before that lets check\n",
    "# if all families has a head. \n",
    "\n",
    "households_head = df_income_train.groupby('idhogar')['parentesco1'].sum()\n",
    "\n",
    "# Find households without a head\n",
    "households_no_head = df_income_train.loc[df_income_train['idhogar'].isin(households_head[households_head == 0].index), :]\n",
    "\n",
    "print('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ef14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find households without a head and where Target value are different\n",
    "households_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "print('{} Households with no head have different Target value.'.format(sum(households_no_head_equal == False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca17a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets fix the data\n",
    "#Set poverty level of the members and the head of the house within a family.\n",
    "# Iterate through each household\n",
    "for household in not_equal.index:\n",
    "    # Find the correct label (for the head of household)\n",
    "    true_target = int(df_income_train[(df_income_train['idhogar'] == household) & (df_income_train['parentesco1'] == 1.0)]['Target'])\n",
    "    \n",
    "    # Set the correct label for all members in the household\n",
    "    df_income_train.loc[df_income_train['idhogar'] == household, 'Target'] = true_target\n",
    "    \n",
    "    \n",
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bdb82",
   "metadata": {},
   "source": [
    "Lets look at the dataset and plot head of household and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = extreme poverty 2 = moderate poverty 3 = vulnerable households 4 = non vulnerable households \n",
    "target_counts = heads['Target'].value_counts().sort_index()\n",
    "target_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d15e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts.plot.bar(figsize = (8, 6),linewidth = 2,edgecolor = 'k',title=\"Target vs Total_Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879e3e5",
   "metadata": {},
   "source": [
    "\n",
    "extreme poverty is the smallest count in the train dataset. The dataset is biased.\n",
    "\n",
    "Lets look at the Squared Variables\n",
    "‘SQBescolari’\n",
    "‘SQBage’\n",
    "‘SQBhogar_total’\n",
    "‘SQBedjefe’\n",
    "‘SQBhogar_nin’\n",
    "‘SQBovercrowding’\n",
    "‘SQBdependency’\n",
    "‘SQBmeaned’\n",
    "‘agesq’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f76705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets remove them\n",
    "print(df_income_train.shape)\n",
    "cols=['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n",
    "        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\n",
    "\n",
    "\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = cols,inplace=True)\n",
    "\n",
    "print(df_income_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81192e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = ['Id', 'idhogar', 'Target']\n",
    "\n",
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone']\n",
    "\n",
    "ind_ordered = ['rez_esc', 'escolari', 'age']\n",
    "\n",
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9902b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for redundant household variables\n",
    "heads = df_income_train.loc[df_income_train['parentesco1'] == 1, :]\n",
    "heads = heads[id_ + hh_bool + hh_cont + hh_ordered]\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4987f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = heads.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "['coopele', 'area2', 'tamhog', 'hhsize', 'hogar_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n",
    "            annot=True, cmap = plt.cm.Accent_r, fmt='.3f');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd3a43",
   "metadata": {},
   "source": [
    "There are several variables here having to do with the size of the house:\n",
    "r4t3, Total persons in the household\n",
    "tamhog, size of the household\n",
    "tamviv, number of persons living in the household\n",
    "hhsize, household size\n",
    "hogar_total, # of total individuals in the household\n",
    "These variables are all highly correlated with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['tamhog', 'hogar_total', 'r4t3']\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = cols,inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for redundant Individual variables\n",
    "ind = df_income_train[id_ + ind_bool + ind_ordered]\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = ind.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is simply the opposite of male! We can remove the male flag.\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = 'male',inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check area1 and area2 also\n",
    "# area1, =1 zona urbana \n",
    "# area2, =2 zona rural \n",
    "#area2 redundant because we have a column indicating if the house is in a urban zone\n",
    "\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = 'area2',inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally lets delete 'Id', 'idhogar'\n",
    "cols=['Id','idhogar']\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = cols,inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a4511",
   "metadata": {},
   "source": [
    "## Predict the accuracy using random forest classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.iloc[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73acb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features=df_income_train.iloc[:,0:-1] # feature without target\n",
    "y_features=df_income_train.iloc[:,-1] # only target\n",
    "print(x_features.shape)\n",
    "print(y_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75257002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_features,y_features,test_size=0.2,random_state=1)\n",
    "rmclassifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6085013",
   "metadata": {},
   "source": [
    "x_features, y_features: The first parameter is the dataset you're selecting to use.\n",
    "train_size: This parameter sets the size of the training dataset. There are three options: None, which is the default, Int, which requires the exact number of samples, and float, which ranges from 0.1 to 1.0.\n",
    "test_size: This parameter specifies the size of the testing dataset. The default state suits the training size. It will be set to 0.25 if the training size is set to default.\n",
    "random_state: The default mode performs a random split using np.random. Alternatively, you can add an integer using an exact number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmclassifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = rmclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa785ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,y_predict))\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_testdata = rmclassifier.predict(df_income_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83930257",
   "metadata": {},
   "source": [
    "## Check the accuracy using random forest with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b813d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "kfold=KFold(n_splits=5,random_state=seed,shuffle=True)\n",
    "\n",
    "rmclassifier=RandomForestClassifier(random_state=10,n_jobs = -1)\n",
    "print(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\n",
    "results=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\n",
    "print(results.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67548b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees= 100\n",
    "\n",
    "rmclassifier=RandomForestClassifier(n_estimators=100, random_state=10,n_jobs = -1)\n",
    "print(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\n",
    "results=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\n",
    "print(results.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e339c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmclassifier.fit(x_features,y_features)\n",
    "labels = list(x_features)\n",
    "feature_importances = pd.DataFrame({'feature': labels, 'importance': rmclassifier.feature_importances_})\n",
    "feature_importances=feature_importances[feature_importances.importance>0.015]\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_testdata = rmclassifier.predict(df_income_test)\n",
    "y_predict_testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "feature_importances['positive'] = feature_importances['importance'] > 0\n",
    "feature_importances.set_index('feature',inplace=True)\n",
    "feature_importances.head()\n",
    "\n",
    "feature_importances.importance.plot(kind='barh', figsize=(11, 6),color = feature_importances.positive.map({True: 'blue', False: 'red'}))\n",
    "plt.xlabel('Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2efc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
